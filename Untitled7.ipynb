{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HalooMo/rep/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "HvEXn1JWdmmg",
        "outputId": "cda9d3c7-3909-4a40-b3a8-799a381bb8c6"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'aclImdb/train/pos'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f524f799e4a1>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1337\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'aclImdb/train/pos'"
          ]
        }
      ],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                          DATA Preparation  (Make Validation DATA PATH)            ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "import pathlib\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "\n",
        "BASE_DIR = pathlib.Path(\"aclImdb\")\n",
        "val_dir = BASE_DIR / \"val\"\n",
        "train_dir = BASE_DIR / \"train\"\n",
        "test_dir = BASE_DIR / \"test\"\n",
        "for category in (\"pos\", \"neg\"):\n",
        "  if not os.path.exists(val_dir/category):\n",
        "    os.makedirs(val_dir/category)\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "  else:\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B99w_bsJUAna",
        "outputId": "6a68982f-d3f4-4b84-d9d4-06e6020c644c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 66000 files belonging to 3 classes.\n",
            "Found 9000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                          DATA Preparation  (Make Validation DATA SETS)       ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import pathlib\n",
        "from keras.datasets import imdb\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "BASE_DIR = pathlib.Path(\"aclImdb\")\n",
        "val_dir = BASE_DIR / \"val\"\n",
        "train_dir = BASE_DIR / \"train\"\n",
        "test_dir = BASE_DIR / \"test\"\n",
        "for category in (\"pos\", \"neg\"):\n",
        "  if not os.path.exists(val_dir/category):\n",
        "    os.makedirs(val_dir/category)\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    ngrams = 1,\n",
        "    max_tokens = 20000,\n",
        "    output_mode=\"multi_hot\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        ")\n",
        "VAL_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    val_dir,\n",
        ")\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    test_dir,\n",
        ")\n",
        "\n",
        "\n",
        "only_text_data = TRAIN_DATASETS.map(lambda x, y: x)\n",
        "text_vectorization.adapt(only_text_data)\n",
        "\n",
        "\n",
        "binary1_grams_train = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary1_grams_val = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary1_grams_test = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0ogsxCpXvWD",
        "outputId": "d1f14bd8-2bf1-456e-cdc5-ae115f95dc1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 62800 files belonging to 3 classes.\n",
            "Found 12200 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.3541 - accuracy: 0.8543 - val_loss: 0.2082 - val_accuracy: 0.9202\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.2358 - accuracy: 0.9106 - val_loss: 0.1714 - val_accuracy: 0.9392\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 11s 14ms/step - loss: 0.2075 - accuracy: 0.9259 - val_loss: 0.1516 - val_accuracy: 0.9490\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 0.1927 - accuracy: 0.9361 - val_loss: 0.1384 - val_accuracy: 0.9554\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 11s 15ms/step - loss: 0.1865 - accuracy: 0.9410 - val_loss: 0.1307 - val_accuracy: 0.9584\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.1744 - accuracy: 0.9447 - val_loss: 0.1201 - val_accuracy: 0.9641\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.1664 - accuracy: 0.9486 - val_loss: 0.1169 - val_accuracy: 0.9651\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 11s 14ms/step - loss: 0.1644 - accuracy: 0.9505 - val_loss: 0.1099 - val_accuracy: 0.9675\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 0.1575 - accuracy: 0.9523 - val_loss: 0.1053 - val_accuracy: 0.9688\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.1560 - accuracy: 0.9553 - val_loss: 0.1030 - val_accuracy: 0.9706\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.1030 - accuracy: 0.9706\n",
            "[0.10297918319702148, 0.9706400036811829]\n"
          ]
        }
      ],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                 Make Model for 1grams                        ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import pathlib\n",
        "from keras.datasets import imdb\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "BASE_DIR = pathlib.Path(\"aclImdb\")\n",
        "val_dir = BASE_DIR / \"val\"\n",
        "train_dir = BASE_DIR / \"train\"\n",
        "test_dir = BASE_DIR / \"test\"\n",
        "for category in (\"pos\", \"neg\"):\n",
        "  if not os.path.exists(val_dir/category):\n",
        "    os.makedirs(val_dir/category)\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "  else:\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "\n",
        "\n",
        "\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    ngrams = 1,\n",
        "    max_tokens = 20000,\n",
        "    output_mode=\"multi_hot\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        ")\n",
        "VAL_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    val_dir,\n",
        ")\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    test_dir,\n",
        ")\n",
        "\n",
        "\n",
        "only_text_data = TRAIN_DATASETS.map(lambda x, y: x)\n",
        "text_vectorization.adapt(only_text_data)\n",
        "\n",
        "\n",
        "binary1_grams_train = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary1_grams_val = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary1_grams_test = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "\n",
        "def get_model(max_tokens=20000, dims=32):\n",
        "  input = keras.Input(shape=(20000,))\n",
        "  x = keras.layers.Dense(dims, activation=\"relu\")(input)\n",
        "  x = keras.layers.Dropout(0.5)(x)\n",
        "  x = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(input, x)\n",
        "  model.compile(\n",
        "      loss = \"binary_crossentropy\",\n",
        "      metrics = [\"accuracy\"]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "callback = keras.callbacks.ModelCheckpoint(\n",
        "    \"ngrams_model_imdb.keras\",\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    binary1_grams_train.cache(),\n",
        "    validation_data = binary1_grams_val.cache(),\n",
        "    callbacks=callback,\n",
        "    epochs = 10\n",
        ")\n",
        "\n",
        "model = keras.models.load_model(\"ngrams_model_imdb.keras\")\n",
        "print(model.evaluate(binary1_grams_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqQtlZ-B-Lfa",
        "outputId": "5f67c13b-fdca-42e1-e275-75ef993a0407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 60240 files belonging to 3 classes.\n",
            "Found 14760 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                 Make Model of SERIALS(ПОСЛНДОВАТЕЛЬНОСТИ)    ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import pathlib\n",
        "from keras.datasets import imdb\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "BASE_DIR = pathlib.Path(\"aclImdb\")\n",
        "val_dir = BASE_DIR / \"val\"\n",
        "train_dir = BASE_DIR / \"train\"\n",
        "test_dir = BASE_DIR / \"test\"\n",
        "for category in (\"pos\", \"neg\"):\n",
        "  if not os.path.exists(val_dir/category):\n",
        "    os.makedirs(val_dir/category)\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "  else:\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=600\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    train_dir, batch_size=256,\n",
        ")\n",
        "VAL_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    val_dir, batch_size=256,\n",
        ")\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    test_dir, batch_size=256,\n",
        ")\n",
        "\n",
        "\n",
        "only_text_data = TRAIN_DATASETS.map(lambda x, y: x)\n",
        "text_vectorization.adapt(only_text_data)\n",
        "\n",
        "\n",
        "binary1_grams_train = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary1_grams_val = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary1_grams_test = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "\n",
        "def get_model(max_tokens=20000, dims=32):\n",
        "  input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "  x = tf.one_hot(input, depth = 20000)\n",
        "  x = keras.layers.Bidirectional(keras.layers.LSTM(dims))(x)\n",
        "  x = keras.layers.Dropout(0.5)(x)\n",
        "  x = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(input, x)\n",
        "  model.compile(\n",
        "      loss = \"binary_crossentropy\",\n",
        "      metrics = [\"accuracy\"]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "callback = keras.callbacks.ModelCheckpoint(\n",
        "    \"ngrams_model_imdb.keras\",\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    binary1_grams_train,\n",
        "    validation_data=binary1_grams_val,\n",
        "    callbacks=callback,\n",
        "    epochs = 10\n",
        ")\n",
        "\n",
        "model = keras.models.load_model(\"ngrams_model_imdb.keras\")\n",
        "print(model.evaluate(binary1_grams_test))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11KHBcjmcJ4y"
      },
      "outputs": [],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                 Make Model of SERILS(...)                    ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import pathlib\n",
        "from keras.datasets import imdb\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "BASE_DIR = pathlib.Path(\"aclImdb\")\n",
        "val_dir = BASE_DIR / \"val\"\n",
        "train_dir = BASE_DIR / \"train\"\n",
        "test_dir = BASE_DIR / \"test\"\n",
        "for category in (\"pos\", \"neg\"):\n",
        "  if not os.path.exists(val_dir/category):\n",
        "    os.makedirs(val_dir/category)\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "  else:\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=600\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    train_dir, batch_size=256,\n",
        ")\n",
        "VAL_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    val_dir, batch_size=256,\n",
        ")\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    test_dir, batch_size=256,\n",
        ")\n",
        "\n",
        "\n",
        "only_text_data = TRAIN_DATASETS.map(lambda x, y: x)\n",
        "text_vectorization.adapt(only_text_data)\n",
        "\n",
        "\n",
        "binary1_grams_train = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary1_grams_val = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary1_grams_test = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "\n",
        "def get_model(max_tokens=20000, dims=32, output_dims=256):\n",
        "  input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "  bidirect = keras.layers.Embedding(input_dim=max_tokens, output_dim=output_dims, mask_zero=True)(input)\n",
        "  x = keras.layers.Bidirectional(keras.layers.LSTM(dims))(bidirect)\n",
        "  x = keras.layers.Dropout(0.5)(x)\n",
        "  x = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(input, x)\n",
        "  model.compile(\n",
        "      loss = \"binary_crossentropy\",\n",
        "      metrics = [\"accuracy\"]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "callback = keras.callbacks.ModelCheckpoint(\n",
        "    \"ngrams_model_imdb.keras\",\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    binary1_grams_train,\n",
        "    validation_data=binary1_grams_val,\n",
        "    callbacks=callback,\n",
        "    epochs = 10\n",
        ")\n",
        "\n",
        "model = keras.models.load_model(\"ngrams_model_imdb.keras\")\n",
        "print(model.evaluate(binary1_grams_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbHkJcCKx4Np"
      },
      "outputs": [],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                     DOWNLOAD Embedding World2Vec             ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcTpS4MLyeBQ"
      },
      "outputs": [],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                     Model with Worl2Vec Embeddings           ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import pathlib\n",
        "from keras.datasets import imdb\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "BASE_DIR = pathlib.Path(\"aclImdb\")\n",
        "val_dir = BASE_DIR / \"val\"\n",
        "train_dir = BASE_DIR / \"train\"\n",
        "test_dir = BASE_DIR / \"test\"\n",
        "for category in (\"pos\", \"neg\"):\n",
        "  if not os.path.exists(val_dir/category):\n",
        "    os.makedirs(val_dir/category)\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "  else:\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "\n",
        "\n",
        "\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens = 20000,\n",
        "    output_mode=\"int\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    train_dir,\n",
        ")\n",
        "VAL_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    val_dir,\n",
        ")\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    test_dir,\n",
        ")\n",
        "\n",
        "\n",
        "only_text_data = TRAIN_DATASETS.map(lambda x, y: x)\n",
        "text_vectorization.adapt(only_text_data)\n",
        "\n",
        "\n",
        "binary1_grams_train = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary1_grams_val = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary1_grams_test = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "\n",
        "\n",
        "max_tokens = 20000\n",
        "embed_dims = 100\n",
        "\n",
        "\n",
        "file_path = \"glove.6B.100d.txt\"\n",
        "embedding_index = {}\n",
        "with open(file_path) as file:\n",
        "  for lines in file.readlines():\n",
        "    word, coefs = lines.split(maxsplit=1)\n",
        "    embedding_index[word] = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "\n",
        "vocab = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocab, range(len(vocab))))\n",
        "embed_vector = np.zeros((max_tokens, embed_dims))\n",
        "for word, i in  word_index.items():\n",
        "  if i < max_tokens:\n",
        "    vector = embedding_index.get(word)\n",
        "  if vector is not None:\n",
        "    embed_vector[i] = vector\n",
        "\n",
        "\n",
        "embed_layers = keras.layers.Embedding(\n",
        "    input_dim=max_tokens,\n",
        "    output_dim=100,\n",
        "    embeddings_initializer=keras.initializers.Constant(embed_vector),\n",
        "    mask_zero = True,\n",
        "    trainable=False)\n",
        "\n",
        "def get_model(max_tokens=20000, dims=32):\n",
        "  input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "  x = embed_layers(input)\n",
        "  x = keras.layers.Bidirectional(keras.layers.LSTM(dims))(x)\n",
        "  x = keras.layers.Dropout(0.5)(x)\n",
        "  x = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(input, x)\n",
        "  model.compile(\n",
        "      loss = \"binary_crossentropy\",\n",
        "      metrics = [\"accuracy\"]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "callback = keras.callbacks.ModelCheckpoint(\n",
        "    \"ngrams_model_imdb.keras\",\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    binary1_grams_train,\n",
        "    validation_data=binary1_grams_val,\n",
        "    callbacks=callback,\n",
        "    epochs = 10\n",
        ")\n",
        "\n",
        "model = keras.models.load_model(\"ngrams_model_imdb.keras\")\n",
        "print(model.evaluate(binary1_grams_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RCcf5ieBtMp"
      },
      "outputs": [],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                     TextVectorization START                  ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import keras\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "dataset = [\"first second third\",\n",
        "           \"fourth fifth sixth\",\n",
        "           \"seventh eighth ninth\"]\n",
        "\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings.lower(string_tensor)\n",
        "  return tf.strings.regex_replace( lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        " return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        "    )\n",
        "\n",
        "\n",
        "text_vectorization.adapt(dataset)\n",
        "print(text_vectorization.get_vocabulary())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHnwqbQpX_Bf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.string()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2EXNGL5NToA"
      },
      "outputs": [],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                     Vectorization BASE Class                  ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import pathlib\n",
        "from keras.datasets import imdb\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "class Vectorization():\n",
        "  def lower(self, dataset):\n",
        "    return [i.lower().split() for i in dataset]\n",
        "\n",
        "  def tokenize(self, dataset):\n",
        "    self.vocabulary = {}\n",
        "    self.reverse_vocabulary = {}\n",
        "    text = self.lower(dataset)\n",
        "    for sentence in text:\n",
        "      for token in sentence:\n",
        "        self.vocabulary[token] = len(self.vocabulary)\n",
        "    self.reverse_vocabulary = dict([ (value, key) for key, value in self.vocabulary.items()])\n",
        "\n",
        "\n",
        "  def encode(self, sentence):\n",
        "    return [ self.vocabulary[i] for i in sentence.split()]\n",
        "\n",
        "  def decode(self, int_serial):\n",
        "    return [ self.reverse_vocabulary[i] for i in int_serial]\n",
        "\n",
        "dataset = [\"first second third\",\n",
        "           \"fourth fifth sixth\",\n",
        "           \"seventh eighth ninth\"]\n",
        "\n",
        "vector = Vectorization()\n",
        "vocab = vector.tokenize(dataset)\n",
        "print(vector.encode(\"fourth fifth sixth\"))\n",
        "print(vector.decode([4,5,6]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "0hyuStcVSF01",
        "outputId": "76ef2efe-f8b8-47c7-9bc8-06775edbff5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3358 files belonging to 2 classes.\n",
            "Found 21642 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "237/782 [========>.....................] - ETA: 1:12:11 - loss: 0.6853 - accuracy: 0.6737"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b267e1bcf520>\u001b[0m in \u001b[0;36m<cell line: 134>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m               )\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m    135\u001b[0m           \u001b[0mint_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                     TransformerEmbedding                     ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import pathlib\n",
        "from keras.datasets import imdb\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import re\n",
        "\n",
        "\n",
        "BASE_DIR = pathlib.Path(\"aclImdb\")\n",
        "val_dir = BASE_DIR / \"val\"\n",
        "train_dir = BASE_DIR / \"train\"\n",
        "test_dir = BASE_DIR / \"test\"\n",
        "for category in (\"pos\", \"neg\"):\n",
        "  if not os.path.exists(val_dir/category):\n",
        "    os.makedirs(val_dir/category)\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "  else:\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens = 20000,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=600\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    train_dir, batch_size=32\n",
        ")\n",
        "VAL_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    val_dir, batch_size=32\n",
        ")\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    test_dir, batch_size=32\n",
        ")\n",
        "\n",
        "\n",
        "only_text_data = TRAIN_DATASETS.map(lambda x, y: x)\n",
        "text_vectorization.adapt(only_text_data)\n",
        "\n",
        "\n",
        "int_train = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_test = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEmbedding(keras.layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, dense_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.dense_dim = dense_dim\n",
        "    self.attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_lay = keras.Sequential([keras.layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                       keras.layers.Dense(embed_dim)])\n",
        "\n",
        "    self.norm1 = keras.layers.LayerNormalization()\n",
        "    self.norm2 = keras.layers.LayerNormalization()\n",
        "\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask[:,tf.newaxis,:]\n",
        "    x = self.attention(inputs, inputs, attention_mask=mask)\n",
        "    x = self.norm1(inputs+x)\n",
        "    d = self.dense_lay(x)\n",
        "    return self.norm2(d+x)\n",
        "\n",
        "  def get_config(self):\n",
        "    conf = super().get_config()\n",
        "    conf.update(\n",
        "        {\n",
        "            \"embed_dim\":self.embed_dim,\n",
        "            \"num_heads\":self.num_heads,\n",
        "            \"dense_dim\":self.dense_dim\n",
        "        }\n",
        "    )\n",
        "    return conf\n",
        "\n",
        "embed_dim = 256\n",
        "dense_dim = 32\n",
        "num_heads = 4\n",
        "vocab_size = 20000\n",
        "\n",
        "\n",
        "\n",
        "input = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = keras.layers.Embedding(vocab_size, embed_dim)(input)\n",
        "x = TransformerEmbedding(embed_dim, num_heads, dense_dim)(x)\n",
        "x = keras.layers.GlobalMaxPooling1D()(x)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "output = keras.layers.Dense(1, \"sigmoid\")(x)\n",
        "model = keras.Model(input, output)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"ENID_LANG.keras\", save_best_only=True)\n",
        "    ]\n",
        "model.compile(\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics = [\"accuracy\"],\n",
        "              )\n",
        "\n",
        "model.fit(\n",
        "          int_train,\n",
        "          epochs=5,\n",
        "          callbacks = callbacks,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1TIS73qgzzX",
        "outputId": "ee475664-cbb9-4bec-a270-a921f079f8c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 884 files belonging to 2 classes.\n",
            "Found 24116 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "405/782 [==============>...............] - ETA: 55:25 - loss: 0.6653 - accuracy: 0.6454"
          ]
        }
      ],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                     PositionalEmbedding                      ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import pathlib\n",
        "from keras.datasets import imdb\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import re\n",
        "\n",
        "\n",
        "BASE_DIR = pathlib.Path(\"aclImdb\")\n",
        "val_dir = BASE_DIR / \"val\"\n",
        "train_dir = BASE_DIR / \"train\"\n",
        "test_dir = BASE_DIR / \"test\"\n",
        "for category in (\"pos\", \"neg\"):\n",
        "  if not os.path.exists(val_dir/category):\n",
        "    os.makedirs(val_dir/category)\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "  else:\n",
        "    files = os.listdir(train_dir/category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    val_size = int(len(files)*0.2)\n",
        "    val_files = files[-val_size:]\n",
        "    for fil in val_files:\n",
        "      shutil.move(train_dir/category/fil,\n",
        "                  val_dir/category/fil)\n",
        "\n",
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens = 20000,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=600\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    train_dir, batch_size=32\n",
        ")\n",
        "VAL_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    val_dir, batch_size=32\n",
        ")\n",
        "TRAIN_DATASETS = keras.utils.text_dataset_from_directory(\n",
        "    test_dir, batch_size=32\n",
        ")\n",
        "\n",
        "\n",
        "only_text_data = TRAIN_DATASETS.map(lambda x, y: x)\n",
        "text_vectorization.adapt(only_text_data)\n",
        "\n",
        "\n",
        "int_train = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_test = TRAIN_DATASETS.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEmbedding(keras.layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, dense_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.dense_dim = dense_dim\n",
        "    self.attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_lay = keras.Sequential([keras.layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                       keras.layers.Dense(embed_dim)])\n",
        "\n",
        "    self.norm1 = keras.layers.LayerNormalization()\n",
        "    self.norm2 = keras.layers.LayerNormalization()\n",
        "\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask[:,tf.newaxis,:]\n",
        "    x = self.attention(inputs, inputs, attention_mask=mask)\n",
        "    x = self.norm1(inputs+x)\n",
        "    d = self.dense_lay(x)\n",
        "    return self.norm2(d+x)\n",
        "\n",
        "  def get_config(self):\n",
        "    conf = super().get_config()\n",
        "    conf.update(\n",
        "        {\n",
        "            \"embed_dim\":self.embed_dim,\n",
        "            \"num_heads\":self.num_heads,\n",
        "            \"dense_dim\":self.dense_dim\n",
        "        }\n",
        "    )\n",
        "    return conf\n",
        "\n",
        "\n",
        "class PositionlEmbedding(keras.layers.Layer):\n",
        "  def __init__(self, embed_dim, max_tokens, sequence_length, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.max_tokens = max_tokens\n",
        "    self.sequence_length = sequence_length\n",
        "    self.embedding = keras.layers.Embedding(self.max_tokens, self.embed_dim)\n",
        "    self.positional_embedding = keras.layers.Embedding(self.sequence_length, self.embed_dim)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    lim = inputs.shape[-1]\n",
        "    period = tf.range(start=0, limit=lim, delta=1)\n",
        "    emb = self.embedding(inputs)\n",
        "    pos_emb = self.positional_embedding(period)\n",
        "    return emb + pos_emb\n",
        "\n",
        "  def get_config(self):\n",
        "    conf = super().get_config()\n",
        "    conf.update(\n",
        "        {\n",
        "            \"embed_dim\":self.embed_dim,\n",
        "            \"sequence_length\":self.sequence_length,\n",
        "            \"max_tokens\":self.max_tokens\n",
        "        }\n",
        "    )\n",
        "    return conf\n",
        "\n",
        "embed_dim = 256\n",
        "dense_dim = 32\n",
        "num_heads = 4\n",
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "\n",
        "input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionlEmbedding(embed_dim, max_tokens, sequence_length)(input)\n",
        "x = TransformerEmbedding(embed_dim, num_heads, dense_dim)(x)\n",
        "x = keras.layers.GlobalMaxPooling1D()(x)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "output = keras.layers.Dense(1, \"sigmoid\")(x)\n",
        "model = keras.Model(input, output)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"ENID_LANG.keras\", save_best_only=True)\n",
        "    ]\n",
        "model.compile(\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics = [\"accuracy\"],\n",
        "              )\n",
        "\n",
        "model.fit(\n",
        "          int_train,\n",
        "          epochs=5,\n",
        "          callbacks = callbacks,\n",
        "          validation_data=int_val\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgnOIZ1WwoEB"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n32K0qt8Ov4Y",
        "outputId": "855699dd-5fcd-4831-cef6-9f6d5a017de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  12.9M      0  0:00:06  0:00:06 --:--:-- 16.5M\n"
          ]
        }
      ],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                     DOWNLOAD aclImdb                         ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Uw144BtJpE",
        "outputId": "3b77d40f-1583-4e00-aeff-298206268670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-30 12:13:19--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.202.207, 173.194.203.207, 74.125.199.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.202.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip.1’\n",
            "\n",
            "\rspa-eng.zip.1         0%[                    ]       0  --.-KB/s               \rspa-eng.zip.1       100%[===================>]   2.52M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-05-30 12:13:20 (196 MB/s) - ‘spa-eng.zip.1’ saved [2638744/2638744]\n",
            "\n",
            "replace spa-eng/_about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                     DOWNLOAD ENGLISH-SPANISH                 ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "\n",
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyI_t6h41uIT"
      },
      "outputs": [],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                     RNN TRANSLATE ENGLISH-SPANISH            ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "\n",
        "file_path = \"spa-eng/spa.txt\"\n",
        "pairs = []\n",
        "file = open(file_path, \"r\")\n",
        "for f in file.readlines():\n",
        "  spanish, english = f.split(\"\\t\")\n",
        "  spanish = \"[start]\" + spanish + \"[end]\"\n",
        "  pairs.append((english, spanish))\n",
        "random.shuffle(pairs)\n",
        "\n",
        "pair_len = int(len(pairs)//2)\n",
        "train = pairs[pair_len:]\n",
        "val = pairs[:pair_len]\n",
        "\n",
        "max_tokens = 20000\n",
        "sequence_length = 20\n",
        "\n",
        "\n",
        "\n",
        "punct = string.punctuation + \"¿\"\n",
        "punct.replace(\"[\", \"\")\n",
        "punct.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "def standard(str_ing):\n",
        "  lower_case = tf.strings.lower(str_ing)\n",
        "  return tf.strings.regex_replace(lower_case, f\"[{re.escape(punct)}]\", \"\")\n",
        "\n",
        "english_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = sequence_length\n",
        ")\n",
        "\n",
        "\n",
        "spanish_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = sequence_length + 1,\n",
        "    standardize = standard,\n",
        ")\n",
        "\n",
        "english = [i[0] for i in pairs]\n",
        "spanish = [i[1] for i in pairs]\n",
        "english_vectorization.adapt(english)\n",
        "spanish_vectorization.adapt(spanish)\n",
        "\n",
        "\n",
        "def format_data(eng,spa):\n",
        "  spa = spanish_vectorization(spa)\n",
        "  eng = english_vectorization(eng)\n",
        "  return ({\n",
        "      \"english\":eng,\n",
        "      \"spanish\":spa[:, :-1],\n",
        "      }, spa[:, 1:])\n",
        "\n",
        "def make_ds(data):\n",
        "  eng, spa = zip(*data)\n",
        "  eng = list(eng)\n",
        "  spa = list(spa)\n",
        "  ds = dataset = tf.data.Dataset.from_tensor_slices((eng, spa))\n",
        "  ds = ds.batch(32)\n",
        "  ds = ds.map(format_data, num_parallel_calls=4)\n",
        "  return ds.shuffle(2020).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_ds(train)\n",
        "val_ds = make_ds(val)\n",
        "\n",
        "embed_dim = 256\n",
        "max_tokens = 20000\n",
        "\n",
        "\n",
        "\n",
        "input_e = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = keras.layers.Embedding(max_tokens, embed_dim, mask_zero=True)(input_e)\n",
        "source =  keras.layers.Bidirectional(keras.layers.GRU(32), merge_mode=\"sum\")(x)\n",
        "\n",
        "\n",
        "input_s = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = keras.layers.Embedding(max_tokens, embed_dim, mask_zero=True)(input_s)\n",
        "x =  keras.layers.GRU(32, return_sequences=True)(x, initial_state=source)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "x =  keras.layers.Dense(max_tokens, \"softmax\")(x)\n",
        "\n",
        "rnn_model = keras.Model([input_e, input_s], x)\n",
        "\n",
        "rnn_model.compile(\n",
        "    \"rmsprop\",\n",
        "    \"sparse_categorical_crossentropy\",\n",
        "    [\"accuracy\"],\n",
        ")\n",
        "\n",
        "rnn_model.fit(\n",
        "    train_ds,\n",
        "    epochs=1,\n",
        "    validation_data=val_ds,\n",
        ")\n",
        "\n",
        "\n",
        "vocab = english_vectorization.get_vocabulary()\n",
        "vocabulary = dict(zip(range(len(vocab)), vocab))\n",
        "\n",
        "def get_sequences(input_sequences):\n",
        "  english = english_vectorization([input_sequences])\n",
        "  span = \"[start]\"\n",
        "  for i in range(20):\n",
        "    spanish = spanish_vectorization([span])\n",
        "    pred = rnn_model.predict([english, spanish])\n",
        "    d = np.argmax(pred[0,i,:])\n",
        "    word = vocabulary[d]\n",
        "    span = \" \" + word\n",
        "    if word == \"[end]\":\n",
        "      break\n",
        "  return span\n",
        "\n",
        "\n",
        "data = [ i[0] for i in pairs]\n",
        "for _ in range(20):\n",
        "  text = random.choice(data)\n",
        "  print(text)\n",
        "  print(\"------------------------------------------------------------------------------------------\")\n",
        "  print(\"------------------------------------------------------------------------------------------\")\n",
        "  print(get_sequences(text))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6brqsH4dA-Sw",
        "outputId": "f1c3b711-0cdb-44b4-c5cb-6f05cf831d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([32], shape=(1,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "print(tf.expand_dims(32, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBbPtS3otkE1"
      },
      "outputs": [],
      "source": [
        "#############################################################################################################################################################\n",
        "#\n",
        "#\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                     PREPAIRE ENGLISH-SPANISH                 ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||                                                                              ||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "#||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "\n",
        "file_path = \"spa-eng/spa.txt\"\n",
        "pairs = []\n",
        "file = open(file_path, \"r\")\n",
        "for f in file.readlines():\n",
        "  spanish, english = f.split(\"\\t\")\n",
        "  spanish = \"[start]\" + spanish + \"[end]\"\n",
        "  pairs.append((english, spanish))\n",
        "random.shuffle(pairs)\n",
        "\n",
        "pair_len = int(len(pairs)//2)\n",
        "train = pairs[pair_len:]\n",
        "val = pairs[:pair_len]\n",
        "\n",
        "max_tokens = 20000\n",
        "sequence_length = 20\n",
        "\n",
        "\n",
        "\n",
        "punct = string.punctuation + \"¿\"\n",
        "punct.replace(\"[\", \"\")\n",
        "punct.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "def standard(str_ing):\n",
        "  lower_case = tf.strings.lower(str_ing)\n",
        "  return tf.strings.regex_replace(lower_case, f\"[{re.escape(punct)}]\", \"\")\n",
        "\n",
        "english_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = sequence_length\n",
        ")\n",
        "\n",
        "\n",
        "spanish_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = sequence_length + 1,\n",
        "    standardize = standard,\n",
        ")\n",
        "\n",
        "english = [i[0] for i in pairs]\n",
        "spanish = [i[1] for i in pairs]\n",
        "english_vectorization.adapt(english)\n",
        "spanish_vectorization.adapt(spanish)\n",
        "\n",
        "\n",
        "def format_data(eng,spa):\n",
        "  spa = spanish_vectorization(spa)\n",
        "  eng = english_vectorization(eng)\n",
        "  return ({\n",
        "      \"english\":eng,\n",
        "      \"spanish\":spa[:, :-1],\n",
        "      }, spa[:, 1:])\n",
        "\n",
        "def make_ds(data):\n",
        "  eng, spa = zip(*data)\n",
        "  eng = list(eng)\n",
        "  spa = list(spa)\n",
        "  ds = dataset = tf.data.Dataset.from_tensor_slices((eng, spa))\n",
        "  ds = ds.batch(32)\n",
        "  ds = ds.map(format_data, num_parallel_calls=4)\n",
        "  return ds.shuffle(2020).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_ds(train)\n",
        "val_ds = make_ds(val)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "authorship_tag": "ABX9TyPrOHwbgOhNzMMnK1T2q/sz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}